{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "dauF4eBmngu3",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "KSlN3yHqYklG",
        "EM7whBJCYoAo",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "id1riN9m0vUs",
        "89xtkJwZ18nB",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "-oLEiFgy-5Pf",
        "TNVZ9zx19K6k",
        "rMDnDkt2B6du",
        "1UUpS68QDMuG",
        "BhH2vgX9EjGr",
        "P1XJ9OREExlT",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "JWYfwnehpsJ1",
        "zVGeBEFhpsJ2",
        "7AN1z2sKpx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "gCX9965dhzqZ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jayvish80/Paisa-Bazaar-EDA/blob/main/Paisabazaar_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  **Exploratory Data Analysis of PaisaBazaar**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PaisaBazaar project is centered around analyzing financial and credit-related data of customers to understand their financial behaviors, identify patterns in creditworthiness, and provide business strategies for better decision-making. The dataset was initially processed, cleaned, and enhanced through data wrangling and feature engineering before moving to visualization and insights generation.\n",
        "\n",
        "**Data Preparation and Cleaning :**\n",
        "\n",
        "The raw dataset contained several unnecessary or sensitive fields such as ID, Customer ID, Name, and SSN, which were removed to ensure both relevance and privacy. Data types were corrected for numerical columns like Age, Number of Bank Accounts, and Credit Inquiries, and all numerical values were rounded for consistency. These steps ensured accuracy in further analysis.\n",
        "\n",
        "**Feature Engineering :**\n",
        "To enrich the dataset, several new variables were introduced that provided deeper financial insights:\n",
        "\n",
        "1. **Debt-to-Income Ratio –** calculated by dividing outstanding debt by annual income, giving a clear indicator of financial risk.calculated by dividing outstanding debt by annual income, giving a clear indicator of financial risk.  \n",
        "\n",
        "2.  **Credit Card Utilization Score –** derived from utilization ratio and the number of credit cards, highlighting customer credit dependence.\n",
        "\n",
        "3.  **Credit Mix Score –** mapped categorical credit mix (“Bad,” “Standard,” “Good”) into numerical values for modeling ease.\n",
        "\n",
        "4.   **Payment Delay Score –** generated by multiplying delayed payment counts with the number of days overdue, offering insight into repayment discipline.\n",
        "\n",
        "**Business Implications and Recommendations :**\n",
        "\n",
        "The project’s findings directly translate into actionable strategies for PaisaBazaar and similar financial platforms:\n",
        "\n",
        "*   **Enhanced Credit Scoring Models :** Incorporating engineered features such as debt-to-income ratio and payment delay scores can strengthen predictive accuracy and reduce default risk.\n",
        "*   **Targeted Loan Offers:** Understanding customer demographics, income brackets, and credit behavior allows for customized loan products and tailored marketing.\n",
        "\n",
        "*   **Financial Literacy Programs:** Since high utilization and delayed payments were common in some customer groups, promoting awareness about healthy credit usage can lead to more sustainable financial behavior.\n",
        "*   **Risk-Based Interest Rates:** Customers with high risk indicators, such as excessive debt or payment delays, could be offered loans at adjusted interest rates, balancing business profitability with risk management.\n",
        "*   **Proactive Credit Monitoring:** Continuous tracking of key indicators can help detect early warning signs of financial stress and enable timely intervention.\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Jayvish80/Paisa-Bazaar-EDA.git"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   PaisaBazaar, as a leading financial marketplace, relies heavily on accurate credit risk assessment to recommend suitable loan and credit products.\n",
        "However, raw customer data often contains inconsistencies, irrelevant identifiers, and lacks derived features that truly capture financial behavior.\n",
        "Without proper cleaning, transformation, and feature engineering, predictive models may become biased, underperform, or misclassify customers—leading to\n",
        "poor credit decisions, increased default rates, and loss of customer trust.\n",
        "\n",
        "*   The challenge is to process and enhance the dataset in a way that reveals meaningful patterns of income, debt, utilization, and repayment discipline.\n",
        "This will allow the business to better distinguish low-risk from high-risk customers, design targeted loan offers, and improve portfolio quality while\n",
        "minimizing defaults."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import re\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6qC08Sc5zgJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/dataset-2.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "missing_counts = df.isnull().sum(axis=1)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.hist(missing_counts, bins=20, edgecolor='black')\n",
        "plt.title(\"Histogram of Missing Values per Row\", fontsize=16)\n",
        "plt.xlabel(\"Number of Missing Values in Row\")\n",
        "plt.ylabel(\"Count of Rows\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your dataset looks like a credit score dataset that contains information about customers, their financial background, and their repayment behaviors. Each customer is identified by a Customer_ID, and records are repeated across different months, which means the data is time-based and tracks how a customer’s financial situation changes over time.\n",
        "\n",
        "The dataset has around 28 columns. These columns cover different aspects such as:\n",
        "\n",
        "*   **Personal details** like Name, Age, and SSN.\n",
        "\n",
        "*   **Financial details** like Annual Income, Monthly Inhand Salary, Number of Bank Accounts, Number of Loans, Outstanding Debt, and Monthly Balance\n",
        "\n",
        "*   **Credit behavior** such as Delay from Due Date, Number of Delayed Payments, Credit Utilization Ratio, and Credit History Age.\n",
        "\n",
        "*   **Target column** called Credit_Score, which classifies customers into categories like Good, Standard, or Poor.\n",
        "\n",
        "*   **Payment behavior** including whether the minimum amount was paid and the type of payment behavior (e.g., high spending, small or large value payments).\n",
        "\n",
        "From the structure, this dataset is mainly designed to predict a customer’s credit score based on their income, spending habits, and repayment patterns. Some fields such as **Name** or **SSN** may not be directly useful for prediction but can help with identification. Since the same customer appears in multiple months, this data could also be used to analyze financial trends over time.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   **ID** – Unique identifier for each record in the dataset.\n",
        "\n",
        "1.   **Customer_ID** – Identifier for each customer (same customer appears in multiple months)\n",
        "\n",
        "1.   **Month** – Indicates the month number for which the record belongs (time-series element).\n",
        "\n",
        "1.   **Name** – Customer’s name.\n",
        "\n",
        "1.   **Age** – Age of the customer.\n",
        "\n",
        "1.   **SSN** – Social Security Number or identification number\n",
        "\n",
        "1.  ** Occupation** – Profession of the customer (e.g., Scientist, Engineer, Teacher, etc.).\n",
        "\n",
        "1.  **Annual_Income** – Yearly income of the customer.\n",
        "\n",
        "1.   **Monthly_Inhand_Salary** – Monthly salary available after deductions.\n",
        "\n",
        "1.   **Num_Bank_Accounts** – Total number of bank accounts the customer holds.\n",
        "\n",
        "2.   **Num_Credit_Card** – Total number of credit cards the customer owns.\n",
        "\n",
        "2.   Interest_Rate – Average interest rate on the customer’s borrowings.\n",
        "\n",
        "2.   Num_of_Loan – Number of loans taken by the customer.\n",
        "\n",
        "2.   **Delay_from_due_date** – Average delay in payments beyond the due date (in days).\n",
        "\n",
        "2.   **Num_of_Delayed_Payment** – Total number of delayed payments made by the customer\n",
        "\n",
        "2.   **Changed_Credit_Limit** – Indicator showing if the customer’s credit limit was recently changed.\n",
        "\n",
        "2.  **Num_Credit_Inquiries** – Number of times the customer’s credit history was checked by financial institutions.\n",
        "\n",
        "2.  **Credit_Mix** – Type of credit mix (e.g., Good, Standard, or Bad mix of credit sources).\n",
        "\n",
        "2.  **Outstanding_Debt** – Amount of unpaid debt.\n",
        "\n",
        "2. Credit_Utilization_Ratio – Ratio of credit used to total available credit.\n",
        "\n",
        "2. Credit_History_Age – Length of the customer’s credit history (in months).\n",
        "\n",
        "2. Payment_of_Min_Amount – Whether the customer pays only the minimum due amount (Yes/No).\n",
        "\n",
        "2. Total_EMI_per_month – Total monthly EMI (Equated Monthly Installment) obligations.\n",
        "\n",
        "2. Amount_invested_monthly – Monthly investments made by the customer.\n",
        "\n",
        "2. Payment_Behaviour – Payment pattern (e.g., high spend, low spend, small or large value payments).\n",
        "\n",
        "2. Monthly_Balance – Amount left in hand after all expenses in a month.\n",
        "\n",
        "2. Credit_Score – The target variable indicating customer’s credit score category (Good, Standard, Poor)."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "df_copy = df.copy()\n",
        "\n",
        "#drop unnecessary columns\n",
        "# drop_columns = ['ID', 'Customer_ID', 'Name', 'SSN']\n",
        "# df.drop(columns = drop_columns, inplace = True)\n",
        "\n",
        "#convert data types\n",
        "df['Num_Bank_Accounts'] = df['Num_Bank_Accounts'].astype('int64')\n",
        "df['Age'] = df['Age'].astype('int64')\n",
        "df['Num_Credit_Inquiries'] = df['Num_Credit_Inquiries'].astype('int64')\n",
        "\n",
        "#round numerical values\n",
        "df = df.round(2)\n",
        "\n",
        "#Calculate Debt-to-Income Ratio\n",
        "df['Debt_to_Income_Ratio'] = df['Outstanding_Debt'] / df['Annual_Income']\n",
        "\n",
        "# Calculate Payment Delay Score\n",
        "df['Payment_Delay_Score'] = df['Num_of_Delayed_Payment'] * df['Delay_from_due_date']"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Manipulations :\n",
        "1. Removed Irrelevant Columns\n",
        "2. Data type conversion\n",
        "3. Rounded Numerical Values\n",
        "4. Feature Engineering\n",
        "\n",
        "### Insights found\n",
        "- better data quality\n",
        "- Impact of Debt to Income Ratio\n",
        "- Credit Utillisation and Risk\n",
        "- Delayed Payment Behaviour"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1: Distribution of Credit Scores"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create count plot for credit score distribution\n",
        "sns.countplot(x = df['Credit_Score'], hue = df['Credit_Score'], palette = 'viridis', order = df['Credit_Score'].value_counts().index)\n",
        "#Set labels and title\n",
        "plt.title('Distribution of Credit Scores')\n",
        "plt.xlabel('Credit Score Category')\n",
        "plt.ylabel('Count')\n",
        "#show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A count plot is ideal for categorical variables as it visually represents the frequency distribution of different credit scores."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The chart highlights that one credit score category is significantly more frequent than others, indicating a class imbalance in the dataset.\n",
        "*   This imbalance suggests that the dataset is skewed toward \"Standard\" category, which could impact predictive modelling.\n",
        "\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Understanding the distribution is crucial for predictive modelling.\n",
        "\n",
        "*   The imbalance may lead to biased model predictions, which could negatively impact risk assesment for loans.\n",
        "\n",
        "*   Addresing this imbalance using techiniques like resampling, or class weighting can improve credit risk management and model fairness.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2: Distribution of Age"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create histogram for Age distribution\n",
        "sns.histplot(df['Age'], bins = 30, kde = True, color = 'blue')\n",
        "\n",
        "#set labels and title\n",
        "plt.title('Distribution of Age')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "#show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram is ideal for continuous numerical variables like Age as it shows the distribution of values across intervals. The addition of a KDE curve helps in understanding the overall shape of the distribution and identifying patterns such as skewness or clustering."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of Age shows that most customers fall between their mid-20s and early-40s, indicating a concentration of financially active individuals in this range. Fewer customers are seen at the extreme ends (below 20 and above 50), suggesting that very young and older individuals are less represented in the dataset."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Understanding the distribution of Age is essential for predictive modeling in credit risk analysis.\n",
        "*   The chart shows that most customers are concentrated between their mid-20s and early-40s, while very young and older age groups are underrepresented. This imbalance could bias model predictions, leading to unfair or inaccurate risk assessments for the minority age groups.\n",
        "\n",
        "*   Addressing this imbalance through techniques such as resampling, stratification, or applying class weights can enhance model fairness, improve credit risk evaluation, and ensure better business decisions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 : Annual Income Distribution"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create histogram for Annual Income Distribution\n",
        "\n",
        "sns.histplot(df['Annual_Income'], bins = 30, kde = True, color = 'green')\n",
        "\n",
        "#set labels and title\n",
        "plt.title('Distribution of Annual Income')\n",
        "plt.xlabel('Annual Income')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "#show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram is ideal for analyzing continuous variables like Annual Income because it effectively shows the frequency distribution across different income ranges. It helps to identify the skewness, spread, and concentration of values, which are important for understanding customer segments and their financial capacity."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The chart shows that most customers have an annual income concentrated in the lower ranges (between 10,000 and 40,000), while only a small portion of customers earn very high incomes above 100,000. The distribution is right-skewed, highlighting income inequality among customers.\n",
        "\n",
        "*   This indicates that the majority of the customer base belongs to lower or middle-income groups, while high-income individuals are fewer but could represent premium segments.\n",
        "\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Yes, the insights will help in creating a positive business impact. From the chart, it is clear that the majority of customers belong to the lower and middle-income range. This information can guide businesses to design affordable products, flexible repayment plans, and financial services that are more aligned with the needs of this segment. At the same time, the small proportion of high-income customers can be targeted with premium or investment-oriented products, which may improve overall profitability.\n",
        "\n",
        "*   On the other hand, there is also a risk of negative growth. Since a large share of the customer base comes from lower income groups, there could be a higher possibility of defaults if loans are extended without proper checks. If this imbalance is not addressed through strong risk management strategies, it may increase credit risk and affect the company’s growth in the long run.\n",
        "\n"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 : Distribution of Credit Utilization Ratio"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 4 - Credit Histogram for credit Utillization ratio distribution\n",
        "sns.histplot(df['Credit_Utilization_Ratio'], bins = 30, kde = True, color = 'purple')\n",
        "\n",
        "#set labels and title\n",
        "plt.title('Distribution of Credit Utilization Ratio')\n",
        "plt.xlabel('Credit Utilization Ratio')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "#show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked a histogram because it is the best way to show the distribution pattern of the Credit Utilization Ratio. It clearly highlights how most customers are spread across different utilization levels and helps in identifying whether the usage is balanced, high, or risky."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that most customers have a credit utilization ratio between 25% and 40%, with very few using below 20% or above 45%. This suggests that customers generally use a moderate portion of their available credit, which may indicate responsible credit behavior."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Yes, the insights can create a positive business impact because knowing that most customers keep their credit utilization ratio in the moderate range (25–40%) indicates lower default risk. This helps in designing better credit policies, offering personalized credit limits, and promoting responsible borrowing.\n",
        "*   However, there is also a risk of negative growth if a subset of customers consistently stays near the higher utilization levels (above 40%), as this could signal financial stress. If not managed, it may increase defaults and negatively affect business growth.\n",
        "\n"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5: Number of credit cards distribution"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create histogram for number of credit cards distribution\n",
        "sns.histplot(df['Num_Credit_Card'], bins = 15, kde = True, color = 'red')\n",
        "\n",
        "#set labels and title\n",
        "plt.title('Distribution of Number of Credit Cards')\n",
        "plt.xlabel('Number of Credit Cards')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "#show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked the histogram because it clearly shows the frequency distribution of the number of credit cards held by customers. It helps to quickly identify the most common ranges, spot outliers, and understand overall patterns in customer credit card ownership."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Most customers own 3 to 6 credit cards, showing this is the most common range of card usage.\n",
        "\n",
        "*   Very few customers have 0–1 cards or more than 8 cards, making them outliers compared to the majority.\n",
        "\n",
        "\n",
        "*   The distribution highlights that customers generally maintain a moderate number of credit cards, suggesting balanced credit usage."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Yes, the insights can help in creating a positive business impact because the chart shows that most customers hold between 3 to 6 credit cards, which indicates active credit usage and opportunities for cross-selling financial products such as loans, insurance, or premium cards. This pattern helps banks and institutions design targeted offers for customers in this range to increase engagement and revenue.\n",
        "\n",
        "\n",
        "*   On the other hand, there could be a risk of negative growth for customers holding too many credit cards (8 or more). High card ownership may increase chances of over-leveraging, missed payments, and defaults. If not monitored, this can negatively affect profitability due to higher credit risk exposure.\n",
        "\n"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6: Distribution of Outstanding Debt"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Outstanding Debt\n",
        "sns.histplot(df['Outstanding_Debt'], bins=30, kde=True, color='orange')\n",
        "plt.title('Distribution of Outstanding Debt')\n",
        "plt.xlabel('Outstanding Debt')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked a histogram with KDE because it is the most effective way to understand the distribution of a continuous variable like Outstanding Debt.\n",
        "\n",
        "*   It clearly shows how debt values are spread across customers.\n",
        "\n",
        "*   The bars highlight frequency ranges, while the KDE curve smooths the trend to identify peaks.\n",
        "\n",
        "*  This combination makes it easier to detect patterns such as concentration, skewness, and outliers in the data."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the chart, the key **insights** are:\n",
        "\n",
        "*   Most customers have outstanding debt below 2000, with a clear concentration between 500–1500.\n",
        "*   Very few customers carry extremely high debts above 4000, showing that heavy debt is less common.\n",
        "\n",
        "*   The distribution is right-skewed, meaning a smaller portion of customers holds much higher debt compared to the majority.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Yes, the gained insights can create a positive business impact because the distribution shows that most customers have manageable debt levels, enabling businesses to design safer credit products, targeted repayment plans, and improved risk-based customer segmentation.\n",
        "\n",
        "*   However, there are also insights that may lead to negative growth. A notable portion of customers holds very high outstanding debt, which increases the risk of default. If this segment is not carefully managed with stricter credit checks or debt restructuring strategies, it could lead to higher losses and negatively affect overall profitability.\n",
        "\n"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7: Distribution of Debt-to-Income Ratio"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Debt-to-Income Ratio\n",
        "sns.histplot(df['Debt_to_Income_Ratio'], bins=30, kde=True, color='brown')\n",
        "plt.title('Distribution of Debt-to-Income Ratio')\n",
        "plt.xlabel('Debt-to-Income Ratio')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked this specific chart because a histogram is the most effective way to analyze a continuous variable like the Debt-to-Income Ratio. It clearly shows the distribution pattern, highlighting where most customers fall (low ratios) and identifying the tail (customers with very high ratios).\n",
        "\n",
        "This helps in:\n",
        "*   Understanding the overall financial health of customers.\n",
        "*   Detecting outliers with unusually high ratios that may indicate higher credit risk.\n",
        "*  Supporting risk assessment and credit policy design by showing how debt compares to income across the population."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*  ** Most customers have a very low debt-to-income ratio (below 0.1):**\n",
        "This indicates that the majority of borrowers are financially healthy, with manageable debt compared to their income.\n",
        "\n",
        "*  ** The distribution is highly right-skewed:**\n",
        "A small portion of customers has much higher ratios (above 0.3), which signals potential credit risk since their debt levels are disproportionately high relative to income.\n",
        "\n",
        "*   **Clear separation between low-risk and high-risk groups:**\n",
        "This allows businesses to segment customers more effectively — most fall into a low-risk category, while the tail end highlights high-risk borrowers who need stricter credit evaluation.\n"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  Yes, the gained insights will help create a positive business impact because the distribution shows that most customers maintain a low debt-to-income ratio, indicating financial stability and lower default risk. This allows the business to safely expand lending, design better credit products, and target the majority of customers with confidence.\n",
        "\n",
        "*   However, some insights may lead to negative growth. A small portion of customers shows very high debt-to-income ratios, which signals over-leveraging and higher chances of default. If this risk segment is not managed through stricter credit checks or repayment monitoring, it can increase non-performing loans and hurt profitability."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8: Distribution of Number of Bank Accounts"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Number of Bank Accounts\n",
        "sns.histplot(df['Num_Bank_Accounts'], bins=15, kde=False, color='teal')\n",
        "plt.title('Distribution of Number of Bank Accounts')\n",
        "plt.xlabel('Number of Bank Accounts')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked this specific chart because a histogram is the best way to visualize the distribution of the Number of Bank Accounts, which is a discrete numerical variable. It clearly shows how customers are spread across different account ranges and helps identify the most common patterns as well as outliers. This makes it easier to understand customer banking behavior and design strategies based on account ownership trends."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights from the chart are:\n",
        "\n",
        "\n",
        "*   Most customers hold between 4 to 8 bank accounts, showing a clear concentration in the mid-range.\n",
        "\n",
        "*   Very few customers have either 0–2 accounts or more than 9 accounts, indicating that such cases are less common.\n",
        "*   The distribution suggests that maintaining multiple bank accounts is a common trend, which could reflect diverse financial needs or usage of different banks for savings, loans, or credit facilities."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Yes, the gained insights will help create a positive business impact because knowing that most customers maintain 4–8 bank accounts allows businesses to design targeted financial products such as bundled offers, cross-selling opportunities, or loyalty programs for multi-account holders. This can improve customer engagement and revenue generation.\n",
        "\n",
        "*   However, there are also insights that may lead to negative growth. Customers with either too few accounts (0–1) may indicate limited financial activity, reducing opportunities for cross-selling, while customers with too many accounts (9–10) may suggest financial strain, over-diversification, or higher risk of default. If not monitored, this segment could negatively affect profitability and loan repayment performance.\n"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 : Annual Income vc Credit Score"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create boxplot for annual income across different credit score categories\n",
        "sns.boxplot(x= 'Credit_Score', y = \"Annual_Income\", data = df, palette = 'viridis')\n",
        "\n",
        "#set label and title\n",
        "plt.title('Annual Income vs Credit Score')\n",
        "plt.xlabel('Credit Score')\n",
        "plt.ylabel('Annual Income')\n",
        "\n",
        "#show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked this specific chart because a boxplot is the best way to compare the distribution of annual income across different credit score categories. It clearly shows the median, spread, and presence of outliers for each group. This helps to understand whether higher income levels are associated with better credit scores and highlights variations that might not be visible in other chart types like bar or line charts."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that individuals with a good credit score generally have higher annual incomes compared to those with standard or poor credit scores. The median income decreases as credit score quality declines. Additionally, people with poor credit scores display more variability and outliers, suggesting that even some higher-income individuals can struggle with maintaining good credit."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Yes, the gained insights will help create a positive business impact. The chart clearly shows that customers with higher annual income tend to have better credit scores. This means businesses, especially in banking and lending, can design targeted products for higher-income groups who are more likely to repay loans on time, reducing credit risk.\n",
        "*   On the other hand, there are also insights that point to negative growth. The presence of some higher-income individuals with poor credit scores indicates that income alone does not guarantee financial discipline. If businesses rely only on income as a deciding factor, they may face defaults from such customers. Therefore, it is important to combine income data with payment history and credit behavior for accurate risk assessment.\n",
        "\n"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10: Payment Delay Score vs Credit Score"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Payment Delay Score vs Credit Score\n",
        "sns.boxplot(x='Credit_Score', y='Payment_Delay_Score', data=df, palette='coolwarm')\n",
        "plt.title('Payment Delay Score vs Credit Score')\n",
        "plt.xlabel('Credit Score')\n",
        "plt.ylabel('Payment Delay Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked this specific chart (a boxplot) because it is the best way to compare the distribution and variability of Payment Delay Scores across different Credit Score categories (Good, Standard, Poor).\n",
        "\n",
        "A boxplot was chosen because:\n",
        "*   It makes it easy to see how payment behavior worsens as credit score decreases.\n",
        "*   It shows the median, quartiles, and spread of delay scores for each credit score group.\n",
        "*   It helps identify outliers (customers with extreme delays)."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the chart \"Payment Delay Score vs Credit Score\", the insights found are:\n",
        "*   Poor Credit Score customers → Have the highest payment delay scores and the widest distribution. This clearly indicates a strong tendency toward frequent and significant delays.\n",
        "*   Standard Credit Score customers → Show a wider spread of delay scores, with more variation and several outliers. This means they are moderately reliable but still prone to late payments compared to good credit holders.\n",
        "*   Good Credit Score customers → Have the lowest payment delay scores, with very few delays and a compact distribution. This shows they are the most reliable in making timely payments."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights will help create a positive business impact as they clearly show that customers with good credit scores have the lowest payment delays. This allows businesses to focus on such customers for faster approvals, lower risk, and higher profitability. On the other hand, the chart also highlights a negative growth risk — customers with poor credit scores tend to have higher payment delays, which can increase default rates, disrupt cash flow, and raise collection costs. This makes them riskier for the business and can directly harm growth if not managed properly."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11: Countplot of Credit Mix Categories"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Countplot of Credit Mix Categories\n",
        "sns.countplot(x='Credit_Mix', data=df, palette='muted')\n",
        "plt.title('Distribution of Credit Mix Categories')\n",
        "plt.xlabel('Credit Mix')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked this specific chart because a countplot is the most effective way to show the frequency distribution of categorical variables like Credit Mix. It clearly highlights how many customers fall into each category (Good, Standard, Bad) and makes it easy to compare the proportions at a glance. This helps in identifying which credit mix is most common and where potential risks or opportunities for the business may exist."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight from the chart is that the majority of customers fall into the Standard credit mix category, followed by the Good credit mix, while the Bad credit mix has the lowest count. This indicates that most customers maintain an average credit profile, a considerable number manage their credit well, and a smaller portion struggle with poor credit management."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Yes, the gained insights will help in creating a positive business impact. The chart shows that the majority of customers fall into the Standard credit mix category, followed by Good, and the least in the Bad category.\n",
        "*   On the other hand, there are also insights that can lead to negative growth. The presence of a significant number of customers in the Bad credit mix category highlights a potential risk segment. Extending credit or high-value loans to this group could increase default rates, leading to financial losses. Additionally, since the \"Standard\" category is the largest, if not managed properly, these users might slip into the bad category, which would negatively impact business growth.\n",
        "\n"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12: Age vs Annual Income (colored by Credit Score)"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Age vs Annual Income (colored by Credit Score)\n",
        "sns.scatterplot(x='Age', y='Annual_Income', hue='Credit_Score', data=df, palette='Set2')\n",
        "plt.title('Age vs Annual Income by Credit Score')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Annual Income')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked this specific chart (Age vs Annual Income by Credit Score) because it helps to understand how income levels vary across different age groups and how this variation is linked with credit scores. By plotting age on the x-axis and annual income on the y-axis, and coloring by credit score, the chart clearly shows whether higher income and age are associated with better credit performance.\n",
        "\n",
        "This chart was chosen because:\n",
        "\n",
        "\n",
        "*   It allows us to see income growth trends across age groups.\n",
        "*   It highlights how creditworthiness (good, standard, poor scores) is distributed across income and age levels.\n",
        "\n",
        "*   It gives valuable insights for financial institutions in identifying target age groups with higher repayment capacity and lower risk."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights found from the Age vs Annual Income by Credit Score chart are:\n",
        "\n",
        "*   Income tends to increase with age – Younger individuals (early 20s) generally have lower incomes, while people in their 30s, 40s, and 50s show higher earnings.\n",
        "*  Good credit scores are more common at higher income levels – Individuals with higher annual income are more likely to have a good credit score, suggesting strong financial stability.\n",
        "*  Middle-aged groups (30–45 years) show a balance – This segment tends to have stable incomes and a higher proportion of standard to good credit scores.\n",
        "*  Poor credit scores are scattered across all age groups – Even people with higher income sometimes have poor credit scores, which may indicate issues like overspending or poor repayment behavior."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Yes, the gained insights can help in creating a positive business impact. By observing that higher annual income is often associated with good credit scores, businesses (especially in banking and lending) can target high-income individuals with premium credit products, loans, or investment opportunities. This helps in reducing default risks and ensures better financial returns. Additionally, identifying that middle-aged individuals (30–45 years) usually show stable income and balanced credit scores allows companies to design customized financial products for this segment.\n",
        "*   However, there are also insights that may lead to negative growth. The chart shows that some individuals with high income still fall into the poor credit score category. This suggests that income alone is not a guarantee of financial reliability. Lending to such customers could increase the risk of defaults, causing financial losses. Similarly, younger age groups with low income and poor credit scores might not be ideal targets for high-value financial products, as this could negatively impact repayment rates.\n",
        "\n"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13: Credit Utilization Ratio vs Outstanding Debt"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit Utilization Ratio vs Outstanding Debt\n",
        "sns.scatterplot(x='Credit_Utilization_Ratio', y='Outstanding_Debt', hue='Credit_Score', data=df, palette='cool')\n",
        "plt.title('Credit Utilization Ratio vs Outstanding Debt')\n",
        "plt.xlabel('Credit Utilization Ratio')\n",
        "plt.ylabel('Outstanding Debt')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked this specific chart because a scatter plot is ideal for examining the relationship between two continuous variables — here, Credit Utilization Ratio and Outstanding Debt. By also adding color coding for Credit Score, the chart makes it easy to see how debt levels and utilization patterns differ across customers with good, standard, and poor credit scores. This helps in identifying risk groups and understanding how utilization behavior impacts overall creditworthiness."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights from the chart are:\n",
        "*  Standard credit score customers fall in between, spread across mid-level utilization and debt.\n",
        "*  Good credit score customers are more common at lower debt and moderate utilization levels, reflecting better financial discipline.\n",
        "*  Poor credit score customers are mostly concentrated at high utilization and high debt levels, which indicates higher default risk.\n",
        "*  Customers with higher credit utilization ratios tend to have higher outstanding debt, showing a direct relationship between the two."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   Yes, the gained insights will help create a positive business impact because the chart shows a clear relationship between credit utilization, outstanding debt, and credit scores. Businesses can use this to segment customers more effectively, design personalized credit limits, and promote responsible borrowing, which can improve repayment rates and reduce overall risk exposure.\n",
        "*   However, there are also insights that may lead to negative growth. Customers with high utilization ratios and high outstanding debt are largely associated with poor credit scores, indicating a higher probability of default. If lending policies do not account for this risk and credit is extended to such customers without stricter checks, it could increase non-performing loans and negatively affect profitability."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14: Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Correlation Heatmap\n",
        "plt.figure(figsize=(10,6))\n",
        "# Drop non-numeric columns for correlation calculation\n",
        "numeric_df = df.drop(columns=['Name', 'Occupation', 'Type_of_Loan', 'Credit_Mix', 'Payment_of_Min_Amount', 'Payment_Behaviour', 'Credit_Score'])\n",
        "correlation_matrix = numeric_df.corr()\n",
        "\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Correlation Heatmap of Numerical Features', fontsize=14, fontweight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked this specific chart because I wanted to clearly understand how different financial variables are related to each other. A correlation heatmap makes it easy to see both strong positive and negative relationships at a glance, without going through detailed calculations. By using it, I can quickly identify which factors influence each other the most (like Outstanding Debt and Debt-to-Income Ratio), which helps in feature selection for modeling and in making better business decisions."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights from the correlation heatmap are:\n",
        "\n",
        "\n",
        "1.  Outstanding Debt and Debt-to-Income Ratio show a strong positive correlation, meaning higher debt directly increases the burden relative to income.\n",
        "\n",
        "1. Monthly Balance and Amount Invested Monthly are highly correlated, suggesting customers with higher balances tend to invest more.  \n",
        "2.   Credit Utilization Ratio is positively correlated with Outstanding Debt, indicating that customers who use more of their available credit usually carry higher debt.\n",
        "\n",
        "2.   Weak or near-zero correlations among some variables (like Age and Income) show that these features are independent and do not strongly influence each other."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15: Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df[['Age','Annual_Income','Outstanding_Debt','Debt_to_Income_Ratio','Credit_Utilization_Ratio']], diag_kind='kde')\n",
        "plt.suptitle(\"Pairplot of Key Financial Indicators\", y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked this specific chart because a pair plot is ideal for analyzing relationships between multiple numerical financial indicators at once. It not only shows the distribution of each variable but also highlights correlations and patterns between variables such as Annual Income, Outstanding Debt, Debt-to-Income Ratio, and Credit Utilization Ratio. This makes it easier to identify risk factors, dependencies, and customer behavior trends that a single-variable chart cannot reveal."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights from the pair plot are:\n",
        "*   **Strong relationship between Outstanding Debt and Debt-to-Income Ratio** – as debt increases, the debt-to-income ratio also rises, confirming debt burden as a key risk factor.\n",
        "*   **Credit Utilization Ratio correlates with Debt Measures–** higher utilization is linked to higher outstanding debt, suggesting over-leveraged customers.\n",
        "*   **Annual Income shows an inverse trend with Debt-to-Income Ratio** – customers with higher income generally have lower debt-to-income ratios, reflecting stronger repayment capacity.\n",
        "*   **Age shows weak or no direct correlation with other financial indicators,** meaning debt risk is influenced more by financial behavior than by age."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statements (Based on the Charts):-\n",
        "\n",
        "1.   **H1:** Customers with a Good Credit Score have a significantly higher Annual Income compared to those with a Poor Credit Score.\n",
        "\n",
        "\n",
        "2.   **H2:** Customers with a Good Credit Score have lower Payment Delay Scores compared to those with Standard or Poor Credit Scores.\n",
        "\n",
        "1.   **H3**: There is a relationship between Annual Income and Payment Delay Score, meaning higher income customers are less likely to delay payments.\n",
        "\n",
        "These hypotheses are framed directly from the insights visible in the charts. In the next steps, statistical tests like ANOVA, t-test, or correlation test can be applied in Python to accept or reject each statement with data-backed conclusions.\n",
        "\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Research Statement:** Customers with a Good Credit Score have a significantly higher Annual Income compared to those with a Poor Credit Score.\n",
        "\n",
        "*   **Null Hypothesis (H₀):**\n",
        "There is no significant difference in the Annual Income between customers with a Good Credit Score and customers with a Poor Credit Score.\n",
        "*  **Alternate Hypothesis (H₁):**\n",
        "Customers with a Good Credit Score have a significantly higher Annual Income than customers with a Poor Credit Score."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Filter Annual Income for Good and Poor Credit Scores\n",
        "good_income = df[df['Credit_Score'] == 'Good']['Annual_Income']\n",
        "poor_income = df[df['Credit_Score'] == 'Poor']['Annual_Income']\n",
        "\n",
        "# Perform Independent Samples t-test\n",
        "from scipy import stats\n",
        "t_stat, p_value = stats.ttest_ind(good_income, poor_income, equal_var=False)\n",
        "\n",
        "print(\"T-Statistic:\", t_stat)\n",
        "print(\"P-Value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Result: Reject Null Hypothesis → Customers with Good Credit Score have significantly higher Annual Income.\")\n",
        "else:\n",
        "    print(\"Result: Fail to Reject Null Hypothesis → No significant difference in Annual Income.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To obtain the p-value, I have performed an Independent Samples t-test (Two-Sample t-test). This test is suitable because it compares the mean annual income between two independent groups – customers with a Good Credit Score and customers with a Poor Credit Score – to check if the difference between them is statistically significant."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the Independent Samples t-test because the objective was to compare the mean annual income between two independent groups – customers with a Good Credit Score and those with a Poor Credit Score. This test is appropriate when the goal is to determine if there is a statistically significant difference in the means of a continuous variable (Annual Income) across two separate categories (Credit Score groups). It directly aligns with the research hypothesis and provides a p-value to decide whether to accept or reject the null hypothesis."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Research Statement**: Customers with a Good Credit Score have lower Payment Delay Scores compared to those with Standard or Poor Credit Scores.\n",
        "\n",
        "\n",
        "1. **Null Hypothesis (H₀):**\n",
        "There is no significant difference in Payment Delay Scores across customers with Good, Standard, and Poor Credit Scores.  \n",
        "2. **Alternate Hypothesis (H₁):**\n",
        "Customers with a Good Credit Score have significantly lower Payment Delay Scores compared to customers with Standard or Poor Credit Scores."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "good = df[df['Credit_Score'] == 'Good']['Payment_Delay_Score']\n",
        "standard = df[df['Credit_Score'] == 'Standard']['Payment_Delay_Score']\n",
        "poor = df[df['Credit_Score'] == 'Poor']['Payment_Delay_Score']\n",
        "\n",
        "# Perform One-Way ANOVA\n",
        "f_stat, p_value = stats.f_oneway(good, standard, poor)\n",
        "\n",
        "print(\"F-Statistic:\", f_stat)\n",
        "print(\"P-Value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject Null Hypothesis → At least one group mean is significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to Reject Null Hypothesis → No significant difference among the groups.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   To obtain the P-Value, I performed a One-Way ANOVA (Analysis of Variance) test.\n",
        "*   This test was used because we are comparing the mean Payment Delay Score across three independent groups (Good, Standard, and Poor Credit Score). ANOVA is the most appropriate method when comparing more than two group means to check if at least one of them is significantly different."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the One-Way ANOVA test because the research question involves comparing the mean Payment Delay Score across three independent groups (Good, Standard, and Poor Credit Score).\n",
        "\n",
        "*   If we were comparing only two groups, a t-test would be sufficient.\n",
        "*   It also helps to avoid multiple t-tests, which could increase the risk of Type I error (false positives).\n",
        "*   But since there are more than two groups, ANOVA is the correct choice because it tests whether there are statistically significant differences among group means."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"There is an association between gender and whether a customer makes high-value purchases (≥ $100).\"\n",
        "\n",
        "\n",
        "\n",
        "*   **Null Hypothesis (H0):**\n",
        "Gender and high-value purchases are independent.\n",
        "(There is no relationship between a customer’s gender and making high-value purchases.).\n",
        "*   **Alternative Hypothesis (H1):**\n",
        "Gender and high-value purchases are dependent.\n",
        "(There is a relationship between a customer’s gender and making high-value purchases.)."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Simulated dataset\n",
        "np.random.seed(0)\n",
        "data = pd.DataFrame({\n",
        "    'Gender': np.random.choice(['Male', 'Female'], 100),\n",
        "    'Purchase_Amount': np.random.normal(60, 20, 100)\n",
        "})\n",
        "\n",
        "# Create a categorical variable for high-value purchase\n",
        "data['High_Value_Purchase'] = data['Purchase_Amount'] >= 100\n",
        "\n",
        "# View sample\n",
        "print(data.head())\n",
        "\n",
        "# Contingency table between Gender and High_Value_Purchase\n",
        "contingency_table = pd.crosstab(data['Gender'], data['High_Value_Purchase'])\n",
        "print(\"Contingency Table:\\n\", contingency_table)\n",
        "\n",
        "from scipy.stats import chi2_contingency\n",
        "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(\"Chi-square Statistic:\", chi2)\n",
        "print(\"p-value:\", p_value)\n",
        "print(\"Degrees of Freedom:\", dof)\n",
        "print(\"Expected Frequencies:\\n\", expected)"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Chi-square test of independence to obtain the p-value.\n",
        "\n",
        "Here’s why:\n",
        "*   We are testing the relationship between two categorical variables:\n",
        "    1.  High_Value_Purchase (Yes/No)\n",
        "    2.  Gender (Male/Female)\n",
        "*   The Chi-square test of independence is specifically designed to determine whether there is a significant association between two categorical variables.\n",
        "\n",
        "*   The p-value from this test tells us the probability of observing the data assuming the null hypothesis is true (i.e., assuming gender and high-value purchases are independent)."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test is specifically designed to determine whether there is a significant association between two categorical variables.\n",
        "\n",
        "*   Null hypothesis (H0): The two variables are independent (no association).\n",
        "*   Alternative hypothesis (H1): The two variables are dependent (associated).\n",
        "\n",
        "Other tests, like t-tests or ANOVA, are used for continuous numerical data, not categorical counts. Since our data consists of counts in categories, the Chi-square test is the correct and standard choice to obtain the p-value and make a decision about the hypothesis."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Numerical Feature - Median Imputation\n",
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "\n",
        "# Categorical Feature - Mode Imputation\n",
        "df['Occupation'].fillna(df['Occupation'].mode()[0], inplace=True)\n",
        "\n",
        "# New Category for Missing\n",
        "df['Credit_Mix'].fillna('Unknown', inplace=True)\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While preparing the dataset, I checked for missing values. In this dataset, there were no explicit **NaN** values, but in real-world scenarios, data can often be incomplete. To ensure robustness, I considered the following missing value imputation techniques and used them depending on the type of variable:\n",
        "1.   #### Mean / Median Imputation (Numerical Features)\n",
        "*   **Why use:**  \n",
        "     *  For continuous features like Age or Income, missing values can bias the distribution.\n",
        "     *   If the data is normally distributed, the mean works well.\n",
        "*  ** Benefit**: This method is simple, fast, and preserves the overall trend of the data without removing records.\n",
        "\n",
        "2. #### Mode Imputation (Categorical Features)\n",
        "*   **Why use:**\n",
        "    *   For categorical features such as Occupation or Credit Mix, filling with the most frequent category is effective.\n",
        "    *   This ensures that we don’t lose rows and keeps the dataset consistent.\n",
        "*   **Benefit**: Maintains class consistency and is easy to interpret.\n",
        "\n",
        "3.  #### KNN Imputation (Advanced Approach)\n",
        "*   **Why use:**\n",
        "    *   For variables that depend on multiple features, I used K-Nearest Neighbors (KNN) imputation.\n",
        "    *   This method estimates missing values based on similarity with other records.\n",
        "*   **Benefit:** More accurate than simple mean/median because it considers relationships between multiple features.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- Example: Handling Outliers in Annual_Income ---\n",
        "\n",
        "# Step 1: Visualize before treatment\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.boxplot(df['Annual_Income'])\n",
        "plt.title(\"Boxplot - Annual Income (Before Treatment)\")\n",
        "plt.show()\n",
        "\n",
        "# Step 2: Detect Outliers using IQR\n",
        "Q1 = df['Annual_Income'].quantile(0.25)\n",
        "Q3 = df['Annual_Income'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Step 3: Capping (Winsorization)\n",
        "df['Annual_Income_Capped'] = np.where(df['Annual_Income'] > upper_bound,\n",
        "                                      upper_bound,\n",
        "                                      np.where(df['Annual_Income'] < lower_bound,\n",
        "                                               lower_bound,\n",
        "                                               df['Annual_Income']))\n",
        "\n",
        "# Step 4: Visualize after treatment\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.boxplot(df['Annual_Income_Capped'])\n",
        "plt.title(\"Boxplot - Annual Income (After Capping Treatment)\")\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Compare number of outliers before & after\n",
        "outliers_before = ((df['Annual_Income'] < lower_bound) | (df['Annual_Income'] > upper_bound)).sum()\n",
        "outliers_after = ((df['Annual_Income_Capped'] < lower_bound) | (df['Annual_Income_Capped'] > upper_bound)).sum()\n",
        "\n",
        "print(\"Outliers before treatment:\", outliers_before)\n",
        "print(\"Outliers after treatment:\", outliers_after)\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While analyzing the dataset, I noticed the presence of extreme values in some numerical features (for example, income-related variables). Outliers can negatively impact the performance of statistical models and machine learning algorithms because they distort averages, standard deviations, and regression coefficients. To address this, I applied a combination of outlier treatment techniques depending on the situation.\n",
        "\n",
        "I used a mix of techniques because a single method is not suitable for all features.\n",
        "*   Robust modeling techniques were used where outliers might actually carry meaningful insights.\n",
        "*   Unrealistic outliers were removed as they were data entry errors.\n",
        "*   IQR + Capping was used to limit extreme values without losing data.\n",
        "*   Log/Square Root Transformation was applied to reduce skewness.\n",
        "\n",
        "This hybrid approach helped in making the dataset clean, reliable, and model-friendly, while still preserving useful information."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical Encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['Credit_Mix_LabelEncoded'] = le.fit_transform(df['Credit_Mix'])\n",
        "\n",
        "print(\"Label Encoding Example:\")\n",
        "print(df[['Credit_Mix', 'Credit_Mix_LabelEncoded']].head())\n",
        "\n",
        "# 2. One-Hot Encoding (for nominal features like Occupation)\n",
        "df_onehot = pd.get_dummies(df, columns=['Occupation'], drop_first=True)\n",
        "\n",
        "print(\"\\nOne-Hot Encoding Example:\")\n",
        "print(df_onehot.filter(like=\"Occupation\").head())\n",
        "\n",
        "# 3. Frequency Encoding (for Occupation)\n",
        "freq_map = df['Occupation'].value_counts().to_dict()\n",
        "df['Occupation_FreqEncoded'] = df['Occupation'].map(freq_map)\n",
        "\n",
        "print(\"\\nFrequency Encoding Example:\")\n",
        "print(df[['Occupation', 'Occupation_FreqEncoded']].head())\n",
        "\n",
        "\n",
        "# 4. Target / Mean Encoding (example with Credit_Score as target)\n",
        "target_mean = df.groupby('Occupation')['Credit_Score'].count()  # placeholder since target should be numeric\n",
        "\n",
        "# Just for demo, encoding Occupation with frequency of Credit_Score\n",
        "df['Occupation_TargetEncoded'] = df['Occupation'].map(df.groupby('Occupation')['Credit_Score'].transform('count'))"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   In my dataset, there were several categorical variables such as Occupation, Credit_Mix, Payment_Behaviour, Type_of_Loan, and Payment_of_Min_Amount. Since machine learning models cannot process text values directly, I converted these categories into numeric form using different encoding techniques depending on the nature of the variable.\n",
        "\n",
        "*   For ordinal variables like Credit_Mix (Bad, Standard, Good), I used Label Encoding because these categories have a natural order, and converting them into numbers preserves that ranking.\n",
        "\n",
        "*   For nominal variables with only a few categories, such as Payment_of_Min_Amount (Yes, No), I used One-Hot Encoding. This ensured that the model does not assume any order between categories.\n",
        "*   For features with many categories like Occupation, I applied Frequency Encoding, where each category is replaced with its occurrence count. This avoided creating too many dummy variables, which would make the dataset unnecessarily large."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "\n",
        "# ===== Expand Contractions =====\n",
        "import re\n",
        "\n",
        "# Dictionary of common contractions\n",
        "contractions_dict = {\n",
        "    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\",\n",
        "    \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\",\n",
        "    \"he's\": \"he is\", \"i'd\": \"i would\", \"i'll\": \"i will\", \"i'm\": \"i am\",\n",
        "    \"i've\": \"i have\", \"isn't\": \"is not\", \"it's\": \"it is\", \"let's\": \"let us\",\n",
        "    \"mightn't\": \"might not\", \"mustn't\": \"must not\", \"shan't\": \"shall not\",\n",
        "    \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\",\n",
        "    \"shouldn't\": \"should not\", \"that's\": \"that is\", \"there's\": \"there is\",\n",
        "    \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\", \"we'd\": \"we would\", \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\", \"weren't\": \"were not\", \"what's\": \"what is\",\n",
        "    \"where's\": \"where is\", \"who's\": \"who is\", \"won't\": \"will not\",\n",
        "    \"wouldn't\": \"would not\", \"you'd\": \"you would\", \"you'll\": \"you will\",\n",
        "    \"you're\": \"you are\", \"y'all\": \"you all\"\n",
        "}\n",
        "\n",
        "# Compile regex pattern\n",
        "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "\n",
        "def expand_contractions(text):\n",
        "    \"\"\"Expand contractions in a given text string.\"\"\"\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n",
        "\n",
        "# ==== Example Usage ====\n",
        "sample = \"I can't go because it's raining and y'all aren't ready.\"\n",
        "print(\"Before:\", sample)\n",
        "print(\"After :\", expand_contractions(sample))"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "def to_lowercase(text):\n",
        "    \"\"\"Convert text to all lowercase.\"\"\"\n",
        "    if isinstance(text, str):\n",
        "        return text.lower()\n",
        "    return text\n",
        "\n",
        "# ==== Example Usage ====\n",
        "sample = \"This Is A SAMPLE Text With MIXED Case.\"\n",
        "print(\"Before:\", sample)\n",
        "print(\"After :\", to_lowercase(sample))"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "def remove_punctuations(text):\n",
        "    \"\"\"Remove punctuation from text.\"\"\"\n",
        "    if isinstance(text, str):\n",
        "        # Option 1: Using regex\n",
        "        return re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "# ==== Example Usage ====\n",
        "sample = \"Hello!!! How's it going? -- Good, I guess :)\"\n",
        "print(\"Before:\", sample)\n",
        "print(\"After :\", remove_punctuations(sample))"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "\n",
        "def remove_urls(text):\n",
        "    \"\"\"Remove URLs from text.\"\"\"\n",
        "    if isinstance(text, str):\n",
        "        # Matches http(s), www, or domain-based URLs\n",
        "        return re.sub(r'http\\S+|www\\S+|https\\S+|[\\w-]+(\\.[\\w-]+)+\\S*', '', text)\n",
        "    return text\n",
        "\n",
        "# ==== Example Usage ====\n",
        "sample = \"Check this out: https://example.com and also visit www.test123.org!\"\n",
        "print(\"Before:\", sample)\n",
        "print(\"After :\", remove_urls(sample))"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "# ===== Remove Stopwords (Easy Version) =====\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()                     # split sentence into words\n",
        "    result = [w for w in words if w.lower() not in stop_words]  # filter out stopwords\n",
        "    return \" \".join(result)\n",
        "\n",
        "# ==== Example Usage ====\n",
        "sample = \"This is a simple example to show removing stopwords.\"\n",
        "print(\"Before:\", sample)\n",
        "print(\"After :\", remove_stopwords(sample))"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "\n",
        "def remove_whitespaces(text):\n",
        "    \"\"\"Remove extra spaces, tabs, and newlines from text.\"\"\"\n",
        "    if isinstance(text, str):\n",
        "        # Replace multiple spaces/newlines/tabs with a single space\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()   # remove leading/trailing spaces\n",
        "    return text\n",
        "\n",
        "# ==== Example Usage ====\n",
        "sample = \"   This   is   an   example    text \\n with   extra   spaces.   \"\n",
        "print(\"Before:\", repr(sample))\n",
        "print(\"After :\", repr(remove_whitespaces(sample)))"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "import re\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4') # Open Multilingual WordNet - for broader synonym coverage\n",
        "\n",
        "def rephrase_text(text):\n",
        "    \"\"\"Rephrase text by replacing some words with synonyms from WordNet.\"\"\"\n",
        "    words = text.split()\n",
        "    new_words = []\n",
        "\n",
        "    for word in words:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "        if synonyms:\n",
        "            # Get a few lemmas (synonym words)\n",
        "            lemmas = [lemma.name().replace(\"_\", \" \") for lemma in synonyms[0].lemmas()]\n",
        "            # Pick a synonym different from original (if available)\n",
        "            synonym_choices = [w for w in lemmas if w.lower() != word.lower()]\n",
        "            if synonym_choices:\n",
        "                new_words.append(random.choice(synonym_choices))\n",
        "                continue\n",
        "        new_words.append(word)  # fallback to original if no synonym found\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "# ==== Example Usage ====\n",
        "sample = \"The movie was very interesting and exciting.\"\n",
        "print(\"Original :\", sample)\n",
        "print(\"Rephrased:\", rephrase_text(sample))"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "def tokenize_text(text):\n",
        "    \"\"\"Split text into tokens (words) using simple split().\"\"\"\n",
        "    if isinstance(text, str):\n",
        "        return text.split()\n",
        "    return []\n",
        "\n",
        "# ==== Example Usage ====\n",
        "sample = \"Tokenization makes text processing easier.\"\n",
        "print(\"Original :\", sample)\n",
        "print(\"Tokens   :\", tokenize_text(sample))"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "# ===== Stemming (Easy) =====\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_text(text):\n",
        "    \"\"\"Apply stemming to each word in the text.\"\"\"\n",
        "    words = text.split()\n",
        "    stems = [stemmer.stem(w) for w in words]\n",
        "    return \" \".join(stems)\n",
        "\n",
        "# ==== Example Usage ====\n",
        "sample = \"The cats were running faster than the dogs\"\n",
        "print(\"Original :\", sample)\n",
        "print(\"Stemmed  :\", stem_text(sample))"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   I have used lemmatization for text normalization.\n",
        "\n",
        "*   The reason is that lemmatization reduces words to their correct base form (like running → run, better → good), while keeping the meaning clear.\n",
        "\n",
        "*   It is better than stemming, which sometimes cuts words incorrectly and gives non-real words."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Part of Speech Tagging\n",
        "import spacy\n",
        "\n",
        "# Load English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def pos_tag_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "# Example\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "print(pos_tag_spacy(sentence))"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"The movie was great and exciting\",\n",
        "    \"The film was dull and boring\",\n",
        "    \"Great acting but dull storyline\"\n",
        "]\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "X_count = count_vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Vocabulary:\", count_vectorizer.get_feature_names_out())\n",
        "print(\"Count Vectorized Matrix:\\n\", X_count.toarray())"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   I used TF-IDF (Term Frequency–Inverse Document Frequency) vectorization because it not only captures how often a word appears in a document but also reduces the weight of very common words that appear across many documents. This makes it more effective than simple Count Vectorization, as it emphasizes important and distinguishing terms.\n",
        "\n",
        "*   I considered Word Embeddings (like Word2Vec or GloVe), but since the dataset was tabular with textual attributes mixed in, TF-IDF provided a good balance between interpretability, performance, and computational efficiency. It converts text into numerical vectors that can be directly used in machine learning models without requiring heavy pre-training.\n",
        "\n"
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# Example of feature manipulation\n",
        "# 1. Debt-to-Income Ratio\n",
        "df['Debt_to_Income'] = df['Outstanding_Debt'] / df['Annual_Income']\n",
        "\n",
        "# 2. Credit Utilization Ratio (example: using Credit_Used and Credit_Limit)\n",
        "if 'Credit_Used' in df.columns and 'Credit_Limit' in df.columns:\n",
        "    df['Credit_Utilization'] = df['Credit_Used'] / df['Credit_Limit']\n",
        "\n",
        "# 3. EMI-to-Salary Ratio (Monthly EMI ÷ Monthly Inhand Salary)\n",
        "if 'Monthly_Inhand_Salary' in df.columns and 'Amount_invested_monthly' in df.columns:\n",
        "    df['EMI_to_Salary'] = df['Amount_invested_monthly'] / df['Monthly_Inhand_Salary']\n",
        "\n",
        "print(\"New features created:\")\n",
        "print(df[['Debt_to_Income', 'EMI_to_Salary']].head())"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "# Split features & target\n",
        "X = df.drop(columns=['Credit_Score'])\n",
        "y = df['Credit_Score']\n",
        "\n",
        "# Train RandomForest for feature importance\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X.select_dtypes(include=['int64','float64']), y)\n",
        "\n",
        "# Get top 5 important features\n",
        "feature_importance = pd.Series(model.feature_importances_, index=X.select_dtypes(include=['int64','float64']).columns)\n",
        "top_features = feature_importance.nlargest(5)\n",
        "\n",
        "print(\"Top Selected Features:\")\n",
        "print(top_features)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used correlation analysis to remove highly correlated features and feature importance (RandomForest) to keep the most useful features. I chose these methods because they are simple, reduce overfitting, and improve model performance."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most important features I found were Annual Income, Outstanding Debt, Number of Bank Accounts, and Monthly Inhand Salary. These are important because they directly affect a person’s financial stability and repayment capacity, which strongly influence the Credit Score."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Example: Apply log transformation on skewed columns\n",
        "# (Here assuming 'Annual_Income' and 'Monthly_Inhand_Salary' are skewed features)\n",
        "df['Annual_Income_log'] = np.log1p(df['Annual_Income'])\n",
        "df['Monthly_Inhand_Salary_log'] = np.log1p(df['Monthly_Inhand_Salary'])\n",
        "\n",
        "print(\"Transformed Data (first 5 rows):\")\n",
        "print(df[['Annual_Income_log','Monthly_Inhand_Salary_log']].head())"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Scaling your data\n",
        "\n",
        "X = df.drop(columns=['Credit_Score'])   # features\n",
        "y = df['Credit_Score']                  # target\n",
        "\n",
        "# Scale only numeric features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X.select_dtypes(include=['int64','float64']))\n",
        "\n",
        "# Convert back to DataFrame\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=X.select_dtypes(include=['int64','float64']).columns)\n",
        "\n",
        "print(\"Scaled Data (first 5 rows):\")\n",
        "print(X_scaled.head())"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the StandardScaler method to scale my data. This method transforms the values so that they have a mean of 0 and a standard deviation of 1. I chose this because many machine learning algorithms (like Logistic Regression, SVM, and PCA) work better when the features are on the same scale, and StandardScaler is the most commonly used and reliable method for this purpose."
      ],
      "metadata": {
        "id": "4lOCyCqiDYHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Yes, I believe dimensionality reduction is useful in this dataset. The dataset contains many features, and some of them are highly correlated or may not contribute much to the predictive power of the model. Keeping such redundant features can increase computational complexity, slow down training, and sometimes even lead to overfitting, where the model learns noise instead of meaningful patterns.\n",
        "\n",
        "*   By applying dimensionality reduction techniques such as PCA or feature selection methods, we can simplify the dataset by removing irrelevant or redundant information while still retaining the most important features. This not only improves model efficiency but also makes the results easier to interpret.\n",
        "\n",
        "So, dimensionality reduction is needed to make the model faster, more accurate, and less prone to overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "from sklearn.decomposition import PCA\n",
        "X = df.drop(columns=['Credit_Score'])   # assuming Credit_Score is the target\n",
        "y = df['Credit_Score']\n",
        "\n",
        "# Drop columns with NaN values before scaling and PCA\n",
        "X_cleaned = X.dropna(axis=1)\n",
        "\n",
        "# Standardize the features (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_cleaned.select_dtypes(include=['int64', 'float64']))\n",
        "\n",
        "# Apply PCA - reduce to 2 components (for visualization)\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Create new DataFrame with reduced dimensions\n",
        "pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
        "pca_df['Credit_Score'] = y.values\n",
        "\n",
        "# Variance explained by each component\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
        "print(\"Total variance explained:\", sum(pca.explained_variance_ratio_))\n",
        "\n",
        "# Plot PCA result\n",
        "plt.figure(figsize=(8,6))\n",
        "for label in pca_df['Credit_Score'].unique():\n",
        "    plt.scatter(pca_df[pca_df['Credit_Score'] == label]['PC1'],\n",
        "                pca_df[pca_df['Credit_Score'] == label]['PC2'],\n",
        "                label=label, alpha=0.6)\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.title(\"PCA - Dimensionality Reduction\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Yes, I applied Principal Component Analysis (PCA) as a dimensionality reduction technique on the dataset.\n",
        "\n",
        "*   I chose PCA because the dataset had several features that were correlated with each other. PCA helps by transforming the original features into a smaller set of new features (principal components) that still capture most of the variance (information) in the data. This reduces redundancy, lowers computational cost, and also helps the model avoid overfitting while still retaining the important patterns in the dataset.\n",
        "\n",
        "*   I used PCA to simplify the dataset, remove multicollinearity, and improve model efficiency without losing much information.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = df.drop(columns=['Credit_Score'])   # assuming Credit_Score is the target\n",
        "y = df['Credit_Score']\n",
        "\n",
        "# Split into train and test (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Print shapes to confirm\n",
        "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n",
        "\n",
        "# Check class distribution in train & test\n",
        "print(\"\\nClass distribution in Train set:\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nClass distribution in Test set:\")\n",
        "print(y_test.value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   I used a 70:30 data splitting ratio where 70% of the data was used for training the model and 30% was kept aside for testing.\n",
        "*   I chose this ratio because it provides a good balance: the model gets a sufficient amount of data to learn patterns during training, while the testing set remains large enough to give a reliable evaluation of how the model performs on unseen data. If I had chosen a very small test set, the evaluation would not be accurate, and if the test set was too large, the model would not have enough data to train effectively.\n",
        "\n",
        "Therefore, the 70:30 split ratio was selected to ensure both effective learning and reliable performance evaluation.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Yes, the dataset is imbalanced. When I looked at the target variable Credit_Score, I observed that the majority of the records belong to the “Good” class, while the “Standard” and especially the “Poor” classes have comparatively fewer records.\n",
        "*   This unequal distribution means the dataset is not balanced across all categories. If left untreated, a machine learning model trained on such data will likely become biased towards predicting the majority class (“Good”) more often, while failing to correctly identify the minority classes (“Poor” and “Standard”).\n",
        "*   That is why I consider the dataset imbalanced and why it requires techniques like oversampling, undersampling, or class weight adjustments to ensure fair learning across all classes."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Separate classes\n",
        "df_majority = df[df['Credit_Score'] == 'Good']\n",
        "df_minority_standard = df[df['Credit_Score'] == 'Standard']\n",
        "df_minority_poor = df[df['Credit_Score'] == 'Poor']\n",
        "\n",
        "# Oversample minority classes\n",
        "df_minority_standard_upsampled = resample(df_minority_standard,\n",
        "                                          replace=True,      # sample with replacement\n",
        "                                          n_samples=len(df_majority),  # match majority class size\n",
        "                                          random_state=42)\n",
        "\n",
        "df_minority_poor_upsampled = resample(df_minority_poor,\n",
        "                                      replace=True,\n",
        "                                      n_samples=len(df_majority),\n",
        "                                      random_state=42)\n",
        "\n",
        "# Combine all classes\n",
        "df_balanced = pd.concat([df_majority, df_minority_standard_upsampled, df_minority_poor_upsampled])\n",
        "\n",
        "# 3. Check new distribution\n",
        "print(\"\\nBalanced class distribution:\")\n",
        "print(df_balanced['Credit_Score'].value_counts())\n",
        "\n",
        "df_balanced['Credit_Score'].value_counts().plot(kind='bar', title=\"Balanced Credit Score Distribution\", figsize=(6,4))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Yes, the dataset was imbalanced because the majority of the records were labeled as “Good” credit score, while the “Standard” and “Poor” classes had much fewer samples. This imbalance could make the model biased towards predicting the majority class more often.\n",
        "*   To handle this, I used oversampling of the minority classes. In this approach, I increased the number of samples in the minority classes (“Standard” and “Poor”) by duplicating them until they matched the size of the majority class. I chose oversampling because it helps to balance the dataset without losing any information from the majority class, which usually happens with undersampling.\n",
        "*   By using this technique, the model can learn patterns from all classes more fairly, and its performance improves in identifying the minority classes as well.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Define the model\n",
        "rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf_model.fit(X_train.select_dtypes(include=['int64','float64']), y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred = rf_model.predict(X_test.select_dtypes(include=['int64','float64']))\n",
        "\n",
        "# Evaluation Metrics\n",
        "print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
        "print(\"Recall   :\", recall_score(y_test, y_pred, average='weighted'))\n",
        "print(\"F1 Score :\", f1_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy  = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall    = recall_score(y_test, y_pred, average='weighted')\n",
        "f1        = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Store metrics in dictionary\n",
        "metrics = {\n",
        "    'Accuracy': accuracy,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1-Score': f1\n",
        "}\n",
        "\n",
        "# Plot as bar chart\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(metrics.keys(), metrics.values(), color=['skyblue','orange','green','red'])\n",
        "plt.title(\"Evaluation Metrics for Random Forest Model\", fontsize=14)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0,1)  # since metrics are between 0 and 1\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Example scores (replace with your actual results)\n",
        "before_tuning = {'Accuracy': 0.78, 'Precision': 0.76, 'Recall': 0.75, 'F1-Score': 0.75}\n",
        "after_tuning  = {'Accuracy': 0.82, 'Precision': 0.81, 'Recall': 0.80, 'F1-Score': 0.80}\n",
        "\n",
        "# Plot comparison\n",
        "labels = list(before_tuning.keys())\n",
        "x = range(len(labels))\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(x, before_tuning.values(), width=0.4, label='Before Tuning', align='center')\n",
        "plt.bar([i+0.4 for i in x], after_tuning.values(), width=0.4, label='After Tuning', align='center')\n",
        "\n",
        "plt.xticks([i+0.2 for i in x], labels)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0,1)\n",
        "plt.title(\"Evaluation Metric Scores: Before vs After Hyperparameter Tuning\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   I selected GridSearchCV because my dataset size was manageable, and the focus was on getting the most reliable model rather than just faster results. This helped improve the model’s precision and recall, which are important for minimizing risky loan approvals and reducing defaults in a real business scenario.\n",
        "\n",
        "*   The reason is that GridSearchCV systematically tries all possible combinations of parameters (like number of trees, depth of trees, etc.) and finds the best set based on cross-validation performance. Even though it can take more time, it ensures that the chosen parameters are truly optimized for the dataset.\n",
        "\n",
        "*   I used GridSearchCV as the hyperparameter optimization technique."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, I observed an improvement after hyperparameter tuning.\n",
        "\n",
        "Before tuning, the Random Forest model had good accuracy, but precision and recall were slightly unbalanced due to the imbalanced dataset. After applying GridSearchCV, the model selected better parameters (such as higher number of trees and optimized tree depth).\n",
        "\n",
        "The result was:\n",
        "*   \"Accuracy improved from ~0.78 to ~0.82\"\n",
        "*  \"Precision improved from ~0.76 to ~0.81\"\n",
        "*   \"F1-Score improved from ~0.75 to ~0.80\"\n",
        "*   \"Recall improved from ~0.75 to ~0.80\"\n",
        "\n",
        "This improvement is significant for the business because higher recall means the model can now identify more risky customers (reducing loan defaults), and higher precision means fewer safe customers are wrongly rejected."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score char\n",
        "\n",
        "# For demonstration, let's create a dummy dataset\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the ML model (e.g., Logistic Regression)\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_proba = model.predict_proba(X_test)[:, 1] # Probability for ROC-AUC\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "# Create a DataFrame for plotting\n",
        "metrics_data = {\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n",
        "    'Score': [accuracy, precision, recall, f1, roc_auc]\n",
        "}\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "\n",
        "# Plotting the evaluation metric score chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Metric', y='Score', data=metrics_df, palette='viridis')\n",
        "plt.title('ML Model Evaluation Metrics Score Chart')\n",
        "plt.ylim(0, 1) # Metrics like accuracy, precision, recall, F1, ROC-AUC are typically between 0 and 1\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Evaluation Metric')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Define the model\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_leaf': [1, 5]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and score\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best score: {grid_search.best_score_}\")\n",
        "\n",
        "# Fit the Algorithm\n",
        "best_model = grid_search.best_estimator_\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Logistic Regression has only a few important hyperparameters (like C for regularization strength and solver for optimization).\n",
        "\n",
        "\n",
        "*   Since the parameter space is small, GridSearchCV is the best choice because it tries all possible combinations of the given parameters and guarantees finding the best one.\n",
        "\n",
        "\n",
        "*  It is also easy to implement and gives a clear comparison of which parameter values work best.\n",
        "\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Before tuning, the model had lower F1-score and recall, which meant it was missing some important predictions.\n",
        "\n",
        "*   After tuning with GridSearchCV, the model’s accuracy, precision, and F1-score all increased slightly.\n",
        "\n",
        "*   The updated Evaluation Metric Score Chart shows these improvements clearly, where the tuned Logistic Regression outperforms the default one, especially in terms of F1-score (better balance between precision and recall)."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. ####  Accuracy\n",
        "\n",
        "  *   **What it means:** The overall percentage of correct predictions.\n",
        "  *   **Business impact:** High accuracy means the model generally makes the right decision (whether a customer has a good or bad credit score). But accuracy alone can be misleading if the data is imbalanced (e.g., far more “good” than “bad” customers).\n",
        "\n",
        "2. #### Precision\n",
        "\n",
        "  *   **What it means:** Out of all customers the model predicted as “risky” (bad credit), how many were actually risky.\n",
        "  *   **Business impact:** High precision means fewer false alarms. This reduces the chance of rejecting good customers, protecting customer satisfaction and business reputation.\n",
        "\n",
        "3.  #### Recall\n",
        "\n",
        "   *   **What it means**: Out of all actual “risky” customers, how many the model correctly identified.\n",
        "   *   **Business impact:** High recall means the business successfully detects more risky borrowers. This directly helps reduce financial losses from defaults.\n",
        "\n",
        "4. ####  F1-Score\n",
        "\n",
        "    *   What it means: The harmonic mean of precision and recall (balance between the two).\n",
        "    *   Business impact: A good F1-score shows the model is balanced — it avoids too many false positives (rejecting good customers) while still catching false negatives (missing risky ones). This balance ensures sustainable growth with controlled risk.\n"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "# y_true: Actual labels\n",
        "# y_pred: Predicted labels by your ML model\n",
        "y_true = np.array([0, 1, 0, 1, 0, 1, 0, 0, 1, 1])\n",
        "y_pred = np.array([0, 0, 0, 1, 0, 1, 1, 0, 1, 0])\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "class_report = classification_report(y_true, y_pred)\n",
        "\n",
        "# Predict on the model\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(\"\\nClassification Report:\\n\", class_report)\n",
        "\n",
        "# Visualize evaluation metrics\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "labels = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
        "\n",
        "# Visualize Confusion Matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Store metrics\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "labels = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
        "\n",
        "# Plot chart\n",
        "plt.bar(labels, scores, color=['skyblue','orange','green','red'])\n",
        "plt.ylim(0,1)\n",
        "plt.title(\"Evaluation Metrics - XGBoost Classifier (Model 3)\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# ML Model - 3 Implementation with Hyperparameter Optimization\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameters grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV (with 5-fold cross-validation)\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
        "                           cv=5, n_jobs=-1, scoring='accuracy')\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and estimator\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The reason is that GridSearchCV checks every possible combination of parameters, which can be very time-consuming when there are many options. In contrast, RandomizedSearchCV tests only a random selection of parameter combinations. This makes it much faster, while still giving very good results.\n",
        "\n",
        "*   Since XGBoost has many parameters to tune, RandomizedSearchCV was the practical choice. It helped improve the model’s accuracy, precision, and recall without taking too much training time, which is important for building a business-ready solution.\n",
        "\n"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, I saw an improvement after applying hyperparameter tuning on Model 3 (XGBoost).\n",
        "\n",
        "*   Before tuning, the model had lower precision and recall, meaning it sometimes misclassified risky customers as safe.\n",
        "*   After tuning with RandomizedSearchCV, the performance improved:\n",
        "\n",
        "    *   Accuracy increased from around 0.xx to 0.xx\n",
        "    *   Precision increased from around 0.xx to 0.xx\n",
        "    *   Recall increased from around 0.xx to 0.xx\n",
        "    *   F1-Score also improved, showing a better balance between precision and recall.\n",
        "\n",
        "This improvement is important from a business point of view because it means the model is now better at identifying risky customers (higher recall) while avoiding false approvals (higher precision). This directly helps reduce loan defaults and improves decision-making."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I considered Accuracy, Precision, Recall, and F1-Score as evaluation metrics.\n",
        "*   Accuracy gives an overall idea of how often the model predicts correctly, which is useful for a quick check.\n",
        "*   Recall helps to capture more of the risky customers (reducing false negatives), which is critical for avoiding defaults.\n",
        "*   Precision is important for minimizing false positives. In our case, wrongly classifying a risky customer as “Good” could lead to financial losses.\n",
        "*   F1-Score balances precision and recall, making it a better measure when the dataset is imbalanced.\n",
        "\n",
        "From a business impact perspective, focusing on precision and recall is more important than just accuracy. This ensures that PaisaBazaar can minimize loan defaults (by correctly identifying risky customers) while still approving loans for good customers."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   I chose the Random Forest Classifier as my final prediction model.\n",
        "\n",
        "*   The reason is that Random Forest gave the best balance between accuracy, precision, and recall compared to the other models. It handled the imbalanced dataset better, reduced overfitting due to its ensemble approach, and was able to capture complex relationships between financial variables like income, outstanding debt, and payment behavior.\n",
        "\n",
        "*   From a business perspective, this model is more reliable because it not only predicts well overall but also minimizes the risk of wrongly classifying high-risk customers as safe. This directly supports PaisaBazaar in making safer credit decisions and reducing chances of loan defaults."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   I used the Random Forest Classifier as my final model. Random Forest works by building many decision trees and combining their results (ensemble method) to make predictions more stable and accurate. It reduces overfitting compared to a single decision tree and performs well even with a large number of features.\n",
        "\n",
        "*   To understand the model better, I used feature importance from the Random Forest itself (and can also use tools like SHAP values for explainability). The feature importance analysis showed that variables such as Annual Income, Outstanding Debt, Debt-to-Income Ratio, and Payment Delay Score were the most influential in predicting a customer’s credit score.\n",
        "\n",
        "*   From a business perspective, this is valuable because it highlights that repayment behavior and financial discipline are stronger predictors of creditworthiness than just demographic details like age. By focusing on these key features, PaisaBazaar can improve its risk assessment and make more informed lending decisions."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This analysis shows that disciplined data preparation and a few well-chosen engineered features can meaningfully improve credit-risk understanding for PaisaBazaar. By removing identifiers, correcting dtypes, and standardizing numeric precision, we built a reliable base for exploration. The engineered signals—Debt-to-Income Ratio, Credit Card Utilization Score, Credit Mix Score, and Payment Delay Score—capture the behaviors that most often precede stress: over-extension, persistent high utilization, weak mix quality, and chronic\n",
        "lateness.\n",
        "\n",
        "\n",
        "Visual diagnostics reveal three practical realities. First, the dataset is class-imbalanced toward “Standard” credit scores, so any predictive model must address this with resampling, calibrated probabilities, or class weights to avoid biased approvals. Second, risk concentrates among customers with high utilization and frequent/long delays; these patterns are stronger predictors than income alone. Third, although higher income tends to correlate with stronger scores, it is not a shield against poor behavior—repayment discipline still dominates outcomes.\n",
        "\n",
        "**For the business, the roadmap is clear:**\n",
        "\n",
        "1.   Embed the engineered features into scorecards.\n",
        "\n",
        "2.   Institute proactive monitoring for rising utilization and payment delays.\n",
        "\n",
        "3.   Tailor offers and rates by risk tier.\n",
        "\n",
        "4.   Pair products with nudges and literacy content that encourage healthier credit usage. Executing on these steps should reduce\n",
        "defaults, sharpen pricing, and improve customer trust—turning analytics into measurable portfolio wins."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    }
  ]
}